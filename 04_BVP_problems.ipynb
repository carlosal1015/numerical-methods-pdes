{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boundary Value Problems:  Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simplest boundary value problem (BVP) we will run into is the one-dimensional version of Poisson's equation\n",
    "$$\n",
    "    u''(x) = f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usually we solve this equation on a finite interval with either Dirichlet or Neumann boundary conditions.  Because there are two derivatives in the equation we need two boundary conditions to solve the PDE (really and ODE in this case) uniquely.  To start let us consider the following basic problem\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    u''(x) = f(x) \\quad \\Omega = [a, b] \\\\\n",
    "    u(a) = \\alpha \\quad u(b) = \\beta.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BVPs of this sort are often the result of looking at the steady-state form of a time dependent PDE.  For instance, if we were considering the steady-state solution to the heat equation\n",
    "$$\n",
    "    u_t(x,t) = \\kappa u_{xx}(x,t) + \\Psi(x,t) \\quad \\Omega = [0, T] \\times [a, b] \\\\\n",
    "    u(x, 0) = u^0(x) \\quad u(a, t) = \\alpha(t) \\quad u(b, t) = \\beta(t)\n",
    "$$\n",
    "we would solve the equation where $u_t = 0$ and arrive at\n",
    "$$\n",
    "    u''(x) = - \\frac{\\Psi}{\\kappa},\n",
    "$$\n",
    "a version of Poisson's equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In higher spatial dimensions the second derivative turns into a Laplacian.  Notation varies for this but all these are equivalent statements:\n",
    "$$\\begin{aligned}\n",
    "    \\nabla^2 u(\\vec{x~}) &= f(\\vec{x~}) \\\\\n",
    "    \\Delta u(\\vec{x~}) &= f(\\vec{x~}) \\\\\n",
    "    \\sum^N_{i=1} u_{x_i x_i} &= f(\\vec{x~}).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-Dimensional Discretization\n",
    "\n",
    "As a first approach to solving the one-dimensional Poisson's equation let's break up the domain into `m` points, often called a *mesh* or *grid*.  Our goal is to approximate the unknown function $u(x)$ as the mesh points $x_i$.  First we can relate the number of mesh points `m` to the distance between with\n",
    "$$\n",
    "    \\Delta x = \\frac{1}{m + 1}.\n",
    "$$\n",
    "The mesh points $x_i$ can be written as\n",
    "$$\n",
    "    x_i = a + i \\Delta x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can let $\\Delta x$ vary and many of the formulas above have only minor modifications but we will leave that for homework.  Notationally we will also adopt the notation\n",
    "$$\n",
    "    U_i \\approx u(x_i)\n",
    "$$\n",
    "so that $U_i$ are the approximate solution at the grid points and retain the lower-case $u$ to denote the true solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To simplify our discussion let's consider the ODE\n",
    "$$\n",
    "    u''(x) = f(x) \\quad \\Omega = [0, 1] \\\\\n",
    "    u(0) = \\alpha \\quad u(1) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Applying the 2nd order, centered difference approximation for the 2nd derivative we have the equation\n",
    "$$\n",
    "    D^2 U_i = \\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1})\n",
    "$$\n",
    "so that we end up with the approximate algebraic expression at every grid point of\n",
    "$$\n",
    "    \\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1}) = f(x_i) \\quad i = 1, 2, 3, \\ldots, m.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note at this point that these algebraic equations are coupled as each $U_i$ depends on its neighbors.  This means we can write these as system of coupled equations\n",
    "$$\n",
    "    A U = F.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Write the system of equations\n",
    "$$\n",
    "    \u001f\\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1}) = f(x_i) \\quad i = 1, 2, 3, \\ldots, m.\n",
    "$$\n",
    "\n",
    "Note the boundary conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    \\frac{1}{\\Delta x^2} \\begin{bmatrix}\n",
    "    -2 &  1 &    &    &    \\\\\n",
    "     1 & -2 &  1 &    &    \\\\\n",
    "       &  1 & -2 &  1 &    \\\\\n",
    "       &    &  1 & -2 &  1 \\\\\n",
    "       &    &    &  1 & -2 \\\\\n",
    "    \\end{bmatrix} \\begin{bmatrix}\n",
    "        U_1 \\\\ U_2 \\\\ U_3 \\\\ U_4 \\\\ U_5\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        f(x_1) - \\frac{\\alpha}{\\Delta x^2} \\\\ f(x_2) \\\\ f(x_3) \\\\ f(x_4) \\\\ f(x_5) - \\frac{\\beta}{\\Delta x^2} \\\\\n",
    "    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "Want to solve the BVP\n",
    "$$\n",
    "    u_{xx} = e^x, \\quad x \\in [0, 1] \\quad \\text{with} \\quad u(0) = 0, \\text{ and } u(1) = 3\n",
    "$$\n",
    "via the construction of a linear system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "    u_{xx} &= e^x \\\\\n",
    "    u_x &= A + e^x \\\\\n",
    "    u &= Ax + B + e^x\\\\\n",
    "    u(0) &= B + 1 = 0 \\Rightarrow B = -1 \\\\\n",
    "    u(1) &= A - 1 + e^{1} = 3 \\Rightarrow A = 4 - e\\\\ \n",
    "    ~\\\\\n",
    "    u(x) &= (4 - e) x - 1 + e^x\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "a = 0.0\n",
    "b = 1.0\n",
    "u_a = 0.0\n",
    "u_b = 3.0\n",
    "f = lambda x: numpy.exp(x)\n",
    "u_true = lambda x: (4.0 - numpy.exp(1.0)) * x - 1.0 + numpy.exp(x)\n",
    "\n",
    "# Descretization\n",
    "m = 10\n",
    "x_bc = numpy.linspace(a, b, m + 2)\n",
    "x = x_bc[1:-1]\n",
    "delta_x = (b - a) / (m + 1)\n",
    "\n",
    "# Construct matrix A\n",
    "A = numpy.zeros((m, m))\n",
    "diagonal = numpy.ones(m) / delta_x**2\n",
    "A += numpy.diag(diagonal * -2.0, 0)\n",
    "A += numpy.diag(diagonal[:-1], 1)\n",
    "A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "# Construct RHS\n",
    "b = f(x)\n",
    "b[0] -= u_a / delta_x**2\n",
    "b[-1] -= u_b / delta_x**2\n",
    "\n",
    "# Solve system\n",
    "U = numpy.empty(m + 2)\n",
    "U[0] = u_a\n",
    "U[-1] = u_b\n",
    "U[1:-1] = numpy.linalg.solve(A, b)\n",
    "\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Analysis\n",
    "\n",
    "A natural question to ask given our approximation $U_i$ is how close this is to the true solution $u(x)$ at the grid points $x_i$.  To address this we will define the error $E$ as\n",
    "$$\n",
    "    E = U - \\widehat{U}\n",
    "$$\n",
    "where $U$ is the vector of the approximate solution and $\\widehat{U}$ is the vector composed of the $u(x_i)$ such that\n",
    "$$\n",
    "    \\widehat{U_i} = u(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leaves $E$ as a vector still so often we ask the question how does the norm of $E$ behave given a particular $\\Delta x$.  For the $\\infty$-norm we would have\n",
    "$$\n",
    "    ||E||_\\infty = \\max_{1 \\leq i \\leq m} |E_i| = \\max_{1 \\leq i \\leq m} |U_i - u(x_i)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we can show that $||E||_\\infty$ goes to zero as $\\Delta x \\rightarrow 0$ we can then claim that the approximate solution $U_i$ at any of the grid points $E_i \\rightarrow 0$.  If we would like to use other norms we often define slightly modified versions of the norms that also contain the grid width $\\Delta x$ where\n",
    "$$\\begin{aligned}\n",
    "    ||E||_1 &= \\Delta x \\sum^m_{i=1} |E_i| \\\\\n",
    "    ||E||_2 &= \\left( \\Delta x \\sum^m_{i=1} |E_i|^2 \\right )^{1/2}\n",
    "\\end{aligned}$$\n",
    "These are referred to as *grid function norms*.\n",
    "\n",
    "The $E$ defined above is known as the *global error*.  One of our main goals throughout this course is to understand how $E$ behaves given other factors as we defined later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Local Truncation Error\n",
    "\n",
    "The *local truncation error* (LTE) can be defined by replacing the approximate solution $U_i$ by the approximate solution $u(x_i)$.  Since the algebraic equations are an approximation to the original BVP, we do not expect that the true solution will exactly satisfy these equations, this resulting difference is the LTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our one-dimensional finite difference approximation from above we have\n",
    "$$\n",
    "    \u001f\\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1}) = f(x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Replacing $U_i$ with $u(x_i)$ in this equation leads to\n",
    "$$\n",
    "    \u001f\\tau_i = \\frac{1}{\\Delta x^2} (u(x_{i+1}) - 2 u(x_i) + u(x_{i-1})) - f(x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this form the LTE is not as useful but if we assume $u(x)$ is smooth we can repalce the $u(x_i)$ with their Taylor series counterparts, similar to what we did for finite differences.  The relevant Taylor series are\n",
    "$$\n",
    "    u(x_{i \\pm 1}) = u(x_i) \\pm u'(x_i) \\Delta x + \\frac{1}{2} u''(x_i) \\Delta x^2 \\pm \\frac{1}{6} u'''(x_i) \\Delta x^3 + \\frac{1}{24} u^{(4)}(x_i) \\Delta x^4 + \\mathcal{O}(\\Delta x^5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leads to an expression for $\\tau_i$ of\n",
    "$$\\begin{aligned}\n",
    "    \u001f\\tau_i &= \\frac{1}{\\Delta x^2} \\left [u''(x_i) \\Delta x^2 + \\frac{1}{12} u^{(4)}(x_i) \\Delta x^4 + \\mathcal{O}(\\Delta x^5) \\right ] - f(x_i) \\\\\n",
    "    &= u''(x_i) + \\frac{1}{12} u^{(4)}(x_i) \\Delta x^2 + \\mathcal{O}(\\Delta x^4) - f(x_i) \\\\\n",
    "    &= \\frac{1}{12} u^{(4)}(x_i) \\Delta x^2 + \\mathcal{O}(\\Delta x^4)\n",
    "\\end{aligned}$$\n",
    "where we note that the true solution would satisfy $u''(x) = f(x)$.\n",
    "\n",
    "As long as $ u^{(4)}(x_i) $ remains finite (smooth) we know that $\\tau_i \\rightarrow 0$ as $\\Delta x \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also write the vector of LTEs as\n",
    "$$\n",
    "    \\tau = A \\widehat{U} - F\n",
    "$$\n",
    "which implies\n",
    "$$\n",
    "    A\\widehat{U} = F + \\tau.\n",
    "$$\n",
    "Note that here it is the exact solution evaluated stencil, not the approximated function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Global Error\n",
    "\n",
    "What we really want to bound is the global error $E$.  To relate the global error and LTE we can substitute $E = U - \\widehat{U}$ into our expression for the LTE to find\n",
    "$$\n",
    "    A E = -\\tau.\n",
    "$$\n",
    "This means that the global error is the solution to the system of equations we defined for the approximation except with $\\tau$ as the forcing function rather than $F$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This also implies that the global error $E$ can be thought of as an approximation to similar BVP as we started with where\n",
    "$$\n",
    "    e''(x) = -\\tau(x) \\quad \\Omega = [0, 1] \\\\\n",
    "    e(0) = 0 \\quad e(1) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can solve this ODE directly by integrating twice since to find to leading order\n",
    "$$\\begin{aligned}\n",
    "    e(x) &\\approx -\\frac{1}{12} \\Delta x^2 u''(x) + \\frac{1}{12} \\Delta x^2 (u''(0) + x (u''(1) - u''(0))) \\\\\n",
    "    &= \\mathcal{O}(\\Delta x^2) \\rightarrow 0 \\quad \\text{as} \\quad \\Delta x \\rightarrow 0.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stability\n",
    "\n",
    "We showed that the continuous analog to $E$, $e(x)$, does in fact go to zero as $\\Delta x \\rightarrow 0$ but what about $E$?  Instead of showing something based on $e(x)$ let's look back at the original system of equations for the global error\n",
    "$$\n",
    "    A_{\\Delta x} E_{\\Delta x} = - \\tau_{\\Delta x}\n",
    "$$\n",
    "where we now denote a particular realization of the system by the corresponding grid spacing $\\Delta x$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we could invert $A_{\\Delta x}$ we could compute $E_{\\Delta x}$ directly.  Assuming that we can and taking an appropriate norm we find\n",
    "$$\\begin{aligned}\n",
    "    E_{\\Delta x} &= (A_{\\Delta x})^{-1} \\tau_{\\Delta x} \\\\\n",
    "    ||E_{\\Delta x}|| &= ||(A_{\\Delta x})^{-1} \\tau_{\\Delta x}|| \\\\\n",
    "    & \\leq ||(A_{\\Delta x})^{-1} ||~|| \\tau_{\\Delta x}||\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that $\\tau_{\\Delta x} \\rightarrow 0$ as $\\Delta x \\rightarrow 0$ already for our example so if we can bound the norm of the matrix $(A_{\\Delta x})^{-1}$ by some constant $C$ for sufficiently small $\\Delta x$ we can then write a bound on the global error of\n",
    "$$\n",
    "    ||E_{\\Delta x}|| \\leq C ||\\tau_{\\Delta x}||\n",
    "$$\n",
    "demonstrating that $E_{\\Delta x} \\rightarrow 0 $ at least as fast as $\\tau_{\\Delta x} \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can generalize this observation to all linear BVP problems by supposing that we have a finite difference approximation to a linear BVP of the form\n",
    "$$\n",
    "    A_{\\Delta x} U_{\\Delta x} = F_{\\Delta x},\n",
    "$$\n",
    "where $\\Delta x$ is the grid spacing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We say the approximation is *stable* if $(A_{\\Delta x})^{-1}$ exists $\\forall \\Delta x < \\Delta x_0$ and there is a constant $C$ such that\n",
    "$$\n",
    "    ||(A_{\\Delta x})^{-1}|| \\leq C \\quad \\forall \\Delta x < \\Delta x_0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consistency\n",
    "\n",
    "A related and important idea for the discretization of any PDE is that it be consistent with the equation we are approximating.  If\n",
    "$$\n",
    "    ||\\tau_{\\Delta x}|| \\rightarrow 0 \\quad \\text{as}\\quad  \\Delta x \\rightarrow 0\n",
    "$$\n",
    "then we say an approximation is *consistent* with the differential equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convergence\n",
    "\n",
    "We now have all the pieces to say something about the global error $E$.  A method is said to be *convergent* if\n",
    "$$\n",
    "    ||E_{\\Delta x}|| \\rightarrow 0 \\quad \\text{as} \\quad \\Delta x \\rightarrow 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If an approximation is both consistent ($||\\tau_{\\Delta x}|| \\rightarrow 0$ as $\\Delta x \\rightarrow 0$) and stable ($||E_{\\Delta x}|| \\leq C ||\\tau_{\\Delta x}||$) then the approximation is convergent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have only derived this in the case of linear BVPs but in fact these criteria for convergence are often found to be true for any finite difference approximation (and beyond for that matter).  This statement of convergence can also often be strengthened to say\n",
    "$$\n",
    "    \\mathcal{O}(\\Delta x^p) \\text{ LTE } + \\text{ stability } \\Rightarrow \\mathcal{O}(\\Delta x^p) \\text{ global error}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out the most difficult part of this process is usually the statement regarding stability.  In the next section we will see for our simple example how we can prove stability in the 2-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stability in the 2-Norm\n",
    "\n",
    "Recalling our definition of stability, we need to show that for our previously defined $A$ that\n",
    "$$\n",
    "    (A_{\\Delta x})^{-1}\n",
    "$$\n",
    "exists and\n",
    "$$\n",
    "    ||(A_{\\Delta x})^{-1}|| \\leq C \\quad \\forall \\Delta x < \\Delta x_0\n",
    "$$\n",
    "for some $C$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we know that $(A_{\\Delta x})^{-1}$ exists?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can assume now that $A$ is in fact invertible but can we bound the norm of the inverse?  Recall that the 2-norm of a symmetric matrix is equal to its spectral radius,\n",
    "$$\n",
    "    ||A||_2 = \\rho(A) = \\max_{1\\leq p \\leq m} |\\lambda_p|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the inverse of $A$ is also symmetric the eigenvalues of $A^{-1}$ are the inverses of the eigenvalues of $A$ implying that\n",
    "$$\n",
    "    ||A^{-1}||_2 = \\rho(A^{-1}) = \\max_{1\\leq p \\leq m} \\left| \\frac{1}{\\lambda_p} \\right| = \\frac{1}{\\max_{1\\leq p \\leq m} \\left| \\lambda_p \\right|}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If none of the $\\lambda_p$ of $A$ are zero for sufficiently small $\\Delta x$ and the rest are finite as $\\Delta x \\rightarrow 0$ we have shown the stability of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The eigenvalues of the matrix $A$ from above can be written as\n",
    "$$\n",
    "    \\lambda_p = \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1)\n",
    "$$\n",
    "with the corresponding eigenvectors $v^p$ \n",
    "$$\n",
    "    v^p_j = \\sin(p \\pi j \\Delta x)\n",
    "$$\n",
    "as the $j$th component with $j = 1, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check that these are in fact the eigenpairs of the matrix $A$\n",
    "$$\n",
    "    \\lambda_p = \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    v^p_j = \\sin(p \\pi j \\Delta x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "    (A v^p)_j &= \\frac{1}{\\Delta x^2} (v^p_{j-1} - 2 v^p_j + v^p_{j+1} ) \\\\\n",
    "    &= \\frac{1}{\\Delta x^2} (\\sin(p \\pi (j-1) \\Delta x) - 2 \\sin(p \\pi j \\Delta x) + \\sin(p \\pi (j+1) \\Delta x) ) \\\\\n",
    "    &= \\frac{1}{\\Delta x^2} (\\sin(p \\pi j \\Delta x) \\cos(p \\pi \\Delta x) - 2 \\sin(p \\pi j \\Delta x) + \\sin(p \\pi j \\Delta x) \\cos(p \\pi \\Delta x) \\\\\n",
    "    &= \\lambda_p v^p_j.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Compute the smallest eigenvalue\n",
    "If we can show that the eigenvalues are away from the origin then we know $||A||_2$ will be bounded.  In this case the eigenvalues are negative so we need to show that they are always strictly less than zero.\n",
    "\n",
    "$$\n",
    "    \\lambda_p = \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1)\n",
    "$$\n",
    "Use a Taylor series to get an idea of how this behaves with respect to $\\Delta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From these expressions we know that smallest eigenvalue is\n",
    "$$\\begin{aligned}\n",
    "    \\lambda_1 &= \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1) \\\\\n",
    "    &= \\frac{2}{\\Delta x^2} \\left (-\\frac{1}{2} p^2 \\pi^2 \\Delta x^2 + \\frac{1}{24} p^4 \\pi^4 \\Delta x^4 + \\mathcal{O}(\\Delta^6) \\right ) \\\\\n",
    "    &= -p^2 \\pi^2 + \\mathcal{O}(\\Delta x^2).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that this also gives us an error bound as this eigenvalue also will also lead to the largest eigenvalue of the inverse matrix.  We can therefore say\n",
    "$$\n",
    "    ||E^{\\Delta x}||_2 \\leq ||(A^{\\Delta x})^{-1}||_2 ||\\tau^{\\Delta x}||_2 \\approx \\frac{1}{\\pi^2} ||\\tau^{\\Delta x}||_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stability in the $\\infty$-Norm\n",
    "\n",
    "The straight forward approach to show that $||E||_\\infty \\rightarrow 0$ as $\\Delta x \\rightarrow 0$ would be to use the matrix bound\n",
    "$$\n",
    "    ||E||_\\infty \\leq \\frac{1}{\\sqrt{\\Delta x}} ||E||_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our example problem we showed that $||E||_2 = \\mathcal{O}(\\Delta x^2)$ so this implies that we at least know that $||E||_\\infty = \\mathcal{O}(\\Delta x^{3/2})$.  This is unfortunate as we expect $||E||_\\infty = \\mathcal{O}(\\Delta x^{2})$ due to the discretization.  In order to alleviate this problem let's go back and consider our definition of stability but this time consider the $\\infty$-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our matrix $A$ can be seen as a number of discrete approximations to *Green's functions* in each column.  This is more broadly applicable later on so we will spend some time reviewing the theory of Green's functions and apply them to our simple example problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Green's Functions\n",
    "\n",
    "Consider the BVP with Dirichlet boundary conditions\n",
    "$$\n",
    "    u''(x) = f(x) \\quad \\Omega = [0, 1] \\\\\n",
    "    u(0) = \\alpha \\quad u(1) = \\beta.\n",
    "$$\n",
    "Pick a fixed point $\\bar{x} \\in \\Omega$, the Green's function $G(x ; \\bar{x})$ solves the BVP above with\n",
    "$$\n",
    "    f(x) = \\delta(x - \\bar{x})\n",
    "$$\n",
    "and $\\alpha = \\beta = 0$.  You could think of this as the result of a steady-state problem of the heat equation with a point-loss of heat somewhere in the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To find the Green's function for our particular problem we can integrate just around the point $\\bar{x}$ near the $\\delta$ function source to find\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\int^{\\bar{x} + \\epsilon}_{\\bar{x} - \\epsilon} u''(x) dx &= \\int^{\\bar{x} + \\epsilon}_{\\bar{x} - \\epsilon} \\delta(x - \\bar{x}) dx \\\\\n",
    "    u'(\\bar{x} + \\epsilon) - u'(\\bar{x} - \\epsilon) &= 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "recalling that by definition the integral of the $\\delta$ function must be 1 if the interval of integration includes $\\bar{x}$.  We see that the jump in the derivative at $\\bar{x}$ from the left and right should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After a bit of algebra we can solve for the Green's function for our model BVP as\n",
    "$$\n",
    "    G(x; \\bar{x}) = \\left \\{ \\begin{aligned}\n",
    "        (\\bar{x} - 1) x & & 0 \\leq x \\leq \\bar{x} \\\\\n",
    "        \\bar{x} (x - 1) & & \\bar{x} \\leq x \\leq 1\n",
    "    \\end{aligned} \\right . .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One imporant property of linear PDEs (or ODEs) in general is that they exhibit the principle of superposition.  The reason we care about this with Green's functions is that if we have a $f(x)$ composed of two $\\delta$ functions, it turns out the solution is the sum of the corresponding two Green's functions.  For instance if\n",
    "$$\n",
    "    f(x) = \\delta(x - 0.25) + 2 \\delta(x - 0.5)\n",
    "$$\n",
    "then\n",
    "$$\n",
    "    u(x) = G(x ; 0.25) + 2 G(x ; 0.5).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This of course can be extended to an infinite number of $\\delta$  functions so that\n",
    "$$\n",
    "    f(x) = \\int^1_0 f(\\bar{x}) \\delta(x - \\bar{x}) d\\bar{x}\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "    u(x) = \\int^1_0 f(\\bar{x}) G(x ; \\bar{x}) d\\bar{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To incorporate the effects of boundary conditions we can continue to add Green's functions to the solution to find the general solution of our original BVP as\n",
    "$$\n",
    "    u(x) = \\alpha (1 - x) + \\beta x + \\int^1_0 f(\\bar{x}) G(x ; \\bar{x}) d\\bar{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So why did we do all this?  Well the Green's function solution representation above can be thought of as a linear operator on the function $f(x)$.  Written in perhaps more familiar terms we have\n",
    "$$\n",
    "    \\mathcal{A} u = f \\quad u = \\mathcal{A}^{-1} f.\n",
    "$$\n",
    "We see now that our linear operator $\\mathcal{A}$ may be the continuous analog to our discrete matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To proceed we will modify our original matrix $A$ into a slightly different version based on the same discretization.  Instead of moving the boundary terms to the right-hand-side of the equation instead we will introduce two new \"unknowns\", called *ghost cells*, that will be placed at the edges of the grid.  We will label these $U_0$ and $U_{m+1}$.  In reality we know the value of these points, they are the boundary conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The modified system then looks like\n",
    "$$\n",
    "    A = \\frac{1}{\\Delta x^2} \\begin{bmatrix}\n",
    "        \\Delta x^2 & 0  \\\\\n",
    "        1 & -2 & 1 \\\\\n",
    "          &  1 & -2 & 1 \\\\\n",
    "          &    & \\ddots & \\ddots & \\ddots \\\\\n",
    "          &    &        &      1 &     -2 & 1 \\\\\n",
    "          &    &        &        &      1 & -2 & 1 \\\\\n",
    "          &    &        &        &        &  0 & \\Delta x^2\n",
    "    \\end{bmatrix} \\quad \\quad U = \\begin{bmatrix}\n",
    "        U_0 \\\\ U_1 \\\\ \\vdots \\\\ U_m \\\\ U_{m+1}\n",
    "    \\end{bmatrix} \\quad \\quad F = \\begin{bmatrix}\n",
    "        \\alpha \\\\ f(x_1) \\\\ \\vdots \\\\ f(x_{m}) \\\\ \\beta\n",
    "    \\end{bmatrix}  \n",
    "$$\n",
    "This has the advantage later that we can implement more general boundary conditions and it isolates the algebraic dependence on the boundary conditions.  The drawbacks are that the matrix no longer has as simple of a form as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's finally turn to the form of the matrix $A^{-1}$.  Introducing a bit more notation, let $A_{j}$ denote the $j$th column and $A_{ij}$ denote the $i$th $j$th element of the matrix $A$.\n",
    "\n",
    "We know that \n",
    "$$\n",
    "    A A^{-1}_j = e_j\n",
    "$$\n",
    "where $e_j$ is the unit vector with $1$ in the $j$th row ($j$th column of the identity matrix).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the above system has some similarities to a discretized version of the Green's function problem.  Here $e_j$ represents the $\\delta$ function, $A$ the original operator, and $A^{-1}_j$ the effect that the $j$th $\\delta$ function (corresponding to the $\\bar{x}$) has on the full solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that we can write down the inverse matrix directly using Green's functions (see LeVeque for the details) but we end up with\n",
    "$$\n",
    "    A^{-1}_{ij} = \\Delta xG(x_i ; x_j) = \\left \\{ \\begin{aligned}\n",
    "        \\Delta x (x_j - 1) x_i, & & i &= 1,2, \\ldots j \\\\\n",
    "        \\Delta x (x_i - 1) x_j, & & i &= j, j+1, \\ldots , m\n",
    "    \\end{aligned} \\right . .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also write the effective right-hand side of our system as\n",
    "$$\n",
    "    F = \\alpha e_0 + \\beta e_{m+1} + \\sum^m_{j=1} f_j e_j\n",
    "$$\n",
    "and finally the solution as\n",
    "$$\n",
    "    U = \\alpha A^{-1}_{0} + \\beta A^{-1}_{m+1} + \\sum^m_{j=1} f_j A^{-1}_{j}\n",
    "$$\n",
    "whose elements are\n",
    "$$\n",
    "    U_i = \\alpha(1 - x_i) + \\beta x_i + \\Delta x \\sum^m_{j=1} f_j G(x_i ; x_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alright, where has all this gotten us?  Well, since we now know what the form of $A^{-1}$ is we may be able to get at the $\\infty$-norm of this matrix.  Recall that the $\\infty$-norm of a matrix (induced from the $\\infty$-norm) for a vector is\n",
    "$$\n",
    "    || C ||_\\infty = \\max_{0\\leq i \\leq m+1} \\sum^{m+1}_{j=0} |C_{ij}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that due to the form of the matrix $A^{-1}$ the first row's sum is \n",
    "\n",
    "$$\n",
    "    \\sum^{m+1}_{j=0} A_{0j}^{-1} = 1\n",
    "$$\n",
    "\n",
    "as is the last rows $A^{-1}_{m+1}$.  We also know that for the other rows $A^{-1}_{i,0} < 1$ and $A^{-1}_{i,m+1} < 1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The intermediate rows are also all bounded as\n",
    "$$\n",
    "    \\sum^{m+1}_{j=0} |A^{-1}_{ij}| \\leq 1 + 1 + m \\Delta x < 3\n",
    "$$\n",
    "using the fact we know that\n",
    "$$\n",
    "    \\Delta x = \\frac{1}{m+1}.\n",
    "$$\n",
    "\n",
    "This completes our stability wanderings as we can now say definitively that\n",
    "$$\n",
    "    ||A^{-1}||_\\infty < 3 \\quad \\forall \\Delta x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neumann Boundary Conditions\n",
    "\n",
    "As mentioned before, we can incorporate other types of boundary conditions into our discretization using the modified version of our matrix.  Let's try to do this for our original problem but with one side having Neumann boundary conditions:\n",
    "$$\n",
    "    u''(x) = f(x) \\quad \\Omega = [-1, 1] \\\\\n",
    "    u(-1) = \\alpha \\quad u'(1) = \\sigma.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "$$\n",
    "    u''(x) = f(x) \\quad \\Omega = [-1, 1] \\\\\n",
    "    u(-1) = \\alpha \\quad u'(1) = \\sigma.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    u(x) = -(5 + e) x - (2 + e + e^{-1}) + e^x\n",
    "$$\n",
    "\n",
    "Explore implementing the Neumann boundary condition by\n",
    "1. using a one-sided 1st order expression,\n",
    "1. using a centered 2nd order expression, and\n",
    "1. using a one-sided 2nd order expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_mixed_1st_order_one_sided(m):\n",
    "    # Problem setup\n",
    "    a = -1.0\n",
    "    b = 1.0\n",
    "    alpha = 3.0\n",
    "    sigma = -5.0\n",
    "    f = lambda x: numpy.exp(x)\n",
    "\n",
    "    # Descretization\n",
    "    x_bc = numpy.linspace(a, b, m + 2)\n",
    "    x = x_bc[1:-1]\n",
    "    delta_x = (b - a) / (m + 1)\n",
    "\n",
    "    # Construct matrix A\n",
    "    A = numpy.zeros((m + 2, m + 2))\n",
    "    diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "    A += numpy.diag(diagonal * -2.0, 0)\n",
    "    A += numpy.diag(diagonal[:-1], 1)\n",
    "    A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "    # Construct RHS\n",
    "    b = f(x_bc)\n",
    "\n",
    "    # Boundary conditions\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = 0.0\n",
    "    A[-1, -1] = 1.0 / (delta_x)\n",
    "    A[-1, -2] = -1.0 / (delta_x)\n",
    "\n",
    "    b[0] = alpha\n",
    "    b[-1] = sigma\n",
    "\n",
    "    # Solve system\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "\n",
    "    return x_bc, U\n",
    "\n",
    "\n",
    "u_true = lambda x: -(5.0 + numpy.exp(1.0)) * x - (2.0 + numpy.exp(1.0) + numpy.exp(-1.0)) + numpy.exp(x)\n",
    "\n",
    "x_bc, U = solve_mixed_1st_order_one_sided(10)\n",
    "    \n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_mixed_2nd_order_centered(m):\n",
    "    # Problem setup\n",
    "    a = -1.0\n",
    "    b = 1.0\n",
    "    alpha = 3.0\n",
    "    sigma = -5.0\n",
    "    f = lambda x: numpy.exp(x)\n",
    "\n",
    "    # Descretization\n",
    "    x_bc = numpy.linspace(a, b, m + 2)\n",
    "    x = x_bc[1:-1]\n",
    "    delta_x = (b - a) / (m + 1)\n",
    "\n",
    "    # Construct matrix A\n",
    "    A = numpy.zeros((m + 2, m + 2))\n",
    "    diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "    A += numpy.diag(diagonal * -2.0, 0)\n",
    "    A += numpy.diag(diagonal[:-1], 1)\n",
    "    A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "    # Construct RHS\n",
    "    b = f(x_bc)\n",
    "\n",
    "    # Boundary conditions\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = 0.0\n",
    "    A[-1, -1] = -1.0 / (delta_x)\n",
    "    A[-1, -2] =  1.0 / (delta_x)\n",
    "\n",
    "    b[0] = alpha\n",
    "    b[-1] = delta_x / 2.0 * f(x_bc[-1]) - sigma\n",
    "\n",
    "    # Solve system\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "\n",
    "    return x_bc, U\n",
    "\n",
    "x_bc, U = solve_mixed_2nd_order_centered(10)\n",
    "    \n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_mixed_2nd_order_one_sided(m):\n",
    "    # Problem setup\n",
    "    a = -1.0\n",
    "    b = 1.0\n",
    "    alpha = 3.0\n",
    "    sigma = -5.0\n",
    "    f = lambda x: numpy.exp(x)\n",
    "    \n",
    "    # Descretization\n",
    "    x_bc = numpy.linspace(a, b, m + 2)\n",
    "    x = x_bc[1:-1]\n",
    "    delta_x = (b - a) / (m + 1)\n",
    "    \n",
    "    # Construct matrix A\n",
    "    A = numpy.zeros((m + 2, m + 2))\n",
    "    diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "    A += numpy.diag(diagonal * -2.0, 0)\n",
    "    A += numpy.diag(diagonal[:-1], 1)\n",
    "    A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "    # Construct RHS\n",
    "    b = f(x_bc)\n",
    "\n",
    "    # Boundary conditions\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = 0.0\n",
    "    A[-1, -1] = 3.0 / (2.0 * delta_x)\n",
    "    A[-1, -2] = -4.0 / (2.0 * delta_x)\n",
    "    A[-1, -3] = 1.0 / (2.0 * delta_x)\n",
    "\n",
    "    b[0] = alpha\n",
    "    b[-1] = sigma\n",
    "\n",
    "    # Solve system\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "\n",
    "    return x_bc, U\n",
    "\n",
    "x_bc, U = solve_mixed_2nd_order_one_sided(10)\n",
    "    \n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "a = -1.0\n",
    "b = 1.0\n",
    "alpha = 3.0\n",
    "sigma = -5.0\n",
    "f = lambda x: numpy.exp(x)\n",
    "u_true = lambda x: -(5.0 + numpy.exp(1.0)) * x - (2.0 + numpy.exp(1.0) + numpy.exp(-1.0)) + numpy.exp(x)\n",
    "\n",
    "# Compute the error as a function of delta_x\n",
    "m_range = numpy.arange(10, 200, 20)\n",
    "delta_x = numpy.empty(m_range.shape)\n",
    "error = numpy.empty((m_range.shape[0], 3))\n",
    "for (i, m) in enumerate(m_range):\n",
    "    \n",
    "    x = numpy.linspace(a, b, m + 2)\n",
    "    delta_x[i] = (b - a) / (m + 1)\n",
    "\n",
    "    # Compute solution\n",
    "    _, U = solve_mixed_1st_order_one_sided(m)\n",
    "    error[i, 0] = numpy.linalg.norm(U - u_true(x), ord=numpy.inf)\n",
    "    _, U = solve_mixed_2nd_order_one_sided(m)\n",
    "    error[i, 1] = numpy.linalg.norm(U - u_true(x), ord=numpy.inf)\n",
    "    _, U = solve_mixed_2nd_order_centered(m)\n",
    "    error[i, 2] = numpy.linalg.norm(U - u_true(x), ord=numpy.inf)\n",
    "    \n",
    "titles = [\"1st Order, One-Sided\", \"2nd Order, Centered\", \"2nd Order, One-Sided\"]\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "for i in range(3):\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    axes.loglog(delta_x, error[:, i], 'ko', label=\"Approx. Derivative\")\n",
    "\n",
    "    axes.loglog(delta_x, order_C(delta_x[0], error[0,i], 1.0) * delta_x**1.0, 'r--', label=\"1st Order\")\n",
    "    axes.loglog(delta_x, order_C(delta_x[0], error[0,i], 2.0) * delta_x**2.0, 'b--', label=\"2nd Order\")\n",
    "    axes.legend(loc=4)\n",
    "    axes.set_title(titles[i])\n",
    "    axes.set_xlabel(\"$\\Delta x$\")\n",
    "    axes.set_ylabel(\"$|u(x) - U|$\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "U = solve_mixed_1st_order_one_sided(10)\n",
    "U = solve_mixed_2nd_order_one_sided(10)\n",
    "U = solve_mixed_2nd_order_centered(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existance and Uniqueness\n",
    "\n",
    "One question that should be asked before embarking upon a numerical solution to any equation is whether the original is *well-posed*.  Well-posedness is defined as a problem that has a unique solution and depends continuously on the input data (inital condition and boundary conditions are examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the BVP we have been exploring but now add strictly Neumann boundary conditions\n",
    "$$\n",
    "    u''(x) = f(x) \\quad \\Omega = [0, 1] \\\\\n",
    "    u'(0) = \\sigma_0 \\quad u'(1) = \\sigma_1.\n",
    "$$\n",
    "We can easily discretize this using one of our methods developed above but we run into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "a = -1.0\n",
    "b = 1.0\n",
    "alpha = 3.0\n",
    "sigma = -5.0\n",
    "f = lambda x: numpy.exp(x)\n",
    "\n",
    "# Descretization\n",
    "m = 50\n",
    "x_bc = numpy.linspace(a, b, m + 2)\n",
    "x = x_bc[1:-1]\n",
    "delta_x = (b - a) / (m + 1)\n",
    "\n",
    "# Construct matrix A\n",
    "A = numpy.zeros((m + 2, m + 2))\n",
    "diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "A += numpy.diag(diagonal * -2.0, 0)\n",
    "A += numpy.diag(diagonal[:-1], 1)\n",
    "A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "# Construct RHS\n",
    "b = f(x_bc)\n",
    "\n",
    "# Boundary conditions\n",
    "A[0, 0] = -1.0 / delta_x\n",
    "A[0, 1] = 1.0 / delta_x\n",
    "A[-1, -1] = -1.0 / (delta_x)\n",
    "A[-1, -2] =  1.0 / (delta_x)\n",
    "\n",
    "b[0] = delta_x / 2.0 * f(x_bc[0]) - alpha\n",
    "b[-1] = delta_x / 2.0 * f(x_bc[-1]) - sigma\n",
    "\n",
    "# Solve system\n",
    "try:\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "except numpy.linalg.LinAlgError as e:\n",
    "    print(e)\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see why $A$ is singular, the constant vector $e = [1, 1, 1, 1, 1,\\ldots, 1]^T$ is in fact in the null-space of $A$.  Our numerical method has actually demonstrated this problem is *ill-posed*!  Indeed, since the boundary conditions are only on the derivatives there are an infinite number of solutions to the BVP (this could also occur if there were no solutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another way to understand why this is the case is to examine this problem again as the steady-state problem originating with the heat equation.  Consider the heat equation with $\\sigma_0 = \\sigma_1 = 0$ and $f(x) = 0$.  This setup would preserve any heat in the rod as none can escape through the ends of the rod.  In fact, the solution to the steady-state problem would simply to redistribute the heat in the rod evenly across the rod based on the initial condition.  We then would have a solution\n",
    "$$\n",
    "    u(x) = \\int^1_0 u^0(x) dx = C.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem comes from the fact that the steady-state problem does not know about this bit of information by itself.  This means that the BVP as it stands could pick out any $C$ and it would be a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution is similar if we had the same setup except $f(x) \\neq 0$.  Now we are either adding or subtracting heat in the rod.  In this case there may not be a steady state at all!  You can actually show that if the addition and subtraction of heat exactly cancels we may in fact have a solution if\n",
    "$$\n",
    "    \\int^1_0 f(x) dx = 0\n",
    "$$\n",
    "which leads again to an infinite number of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Linear Second Order Discretization\n",
    "\n",
    "Let's now describe a method for solving the equation\n",
    "$$\n",
    "    a(x) u''(x) + b(x) u'(x) + c(x) u(x) = f(x) \\quad \\Omega = [a, b] \\\\\n",
    "    u(a) = \\alpha \\quad u(b) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try discretizing this using second order finite differences and write the system for\n",
    "$$\n",
    "    a(x) u''(x) + b(x) u'(x) + c(x) u(x) = f(x) \\quad \\Omega = [a, b] \\\\\n",
    "    u(a) = \\alpha \\quad u(b) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The general, second order finite difference approximation to the above equation can be written as\n",
    "$$\n",
    "    a_i \\frac{U_{i+1} - 2 U_i + U_{i-1}}{\\Delta x^2} + b_i \\frac{U_{i+1} - U_{i-1}}{2 \\Delta x} + c_i U_i = f_i\n",
    "$$\n",
    "leading to the matrix entries\n",
    "$$\n",
    "    A_{i,i} = -\\frac{2 a_i}{\\Delta x^2} + c_i\n",
    "$$\n",
    "on the diagonal and\n",
    "$$\n",
    "    A_{i,i\\pm1} = \\frac{a_i}{\\Delta x^2} \\pm \\frac{b_i}{2 \\Delta x}\n",
    "$$\n",
    "on the sub-diagonals.  We can take of the boundary conditions by either using the ghost-points approach or by incorporating them into the right hand side evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example:\n",
    "\n",
    "Consider the steady-state heat conduction problem with a variable $\\kappa(x)$ so that\n",
    "$$\n",
    "    (\\kappa(x) u'(x))' = f(x), \\quad \\Omega = [0, 1] \\\\\n",
    "    u(0) = \\alpha \\quad u(1) = \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By the chain rule we know\n",
    "$$\n",
    "    \\kappa(x) u''(x) + \\kappa'(x) u'(x) = f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that in this case this approach is not really the best approach to solving the problem.  In many cases it is best to discretize the original form of the physics rather than a perhaps equivalent formulation.  To demonstrate this let's try to construct a system to solve the original equations\n",
    "$$\n",
    "    (\\kappa(x) u'(x))' = f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we will approximate the expression\n",
    "$$\n",
    "    \\kappa(x) u'(x)\n",
    "$$\n",
    "but at the points half-way in between the points $x_i$, i.e. $x_{i + 1/2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also will take this approximation effectively to be $\\Delta x / 2$ and find\n",
    "$$\n",
    "    \\kappa(x_{i+1/2}) u'(x_{i+1/2}) = \\kappa_{i+1/2} \\frac{U_{i+1} - U_i}{\\Delta x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now taking this approximation and differencing it with the same difference centered at $x_{i-1/2}$ leads to\n",
    "$$\\begin{aligned}\n",
    "    (\\kappa(x_i) u'(x_i))' &= \\frac{1}{\\Delta x} \\left [ \\kappa_{i+1/2} \\frac{U_{i+1} - U_i}{\\Delta x} - \\kappa_{i-1/2} \\frac{U_{i} - U_{i-1}}{\\Delta x} \\right ] \\\\\n",
    "    &= \\frac{\\kappa_{i+1/2}U_{i+1} - \\kappa_{i+1/2} U_i -\\kappa_{i-1/2} U_{i} + \\kappa_{i-1/2} U_{i-1}}{\\Delta x^2} \\\\ \n",
    "    &= \\frac{\\kappa_{i+1/2}U_{i+1} - (\\kappa_{i+1/2} - \\kappa_{i-1/2}) U_i + \\kappa_{i-1/2} U_{i-1}}{\\Delta x^2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that these formulations are actually equivalent to $\\mathcal{O}(\\Delta x^2)$.  The matrix entries are\n",
    "$$\\begin{aligned}\n",
    "    A_{i,i} = -\\frac{\\kappa_{i+1/2} - \\kappa_{i-1/2}}{\\Delta x^2} \\\\\n",
    "    A_{i,i \\pm 1} = \\frac{\\kappa_{i\\pm 1/2}}{\\Delta x^2}.\n",
    "\\end{aligned}$$\n",
    "Note that this latter discretization is symmetric.  This will have consequences as to how well or quickly we can solve the resulting system of linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-Linear Equations\n",
    "\n",
    "Our model problem, Poisson's equation, is a linear BVP.  How would we approach a non-linear problem?  As a new model problem let's consider the non-linear pendulum problem.  The physical system is a mass $m$ connected to a rigid, massless rod of length $L$ which is allowed to swing about a point.  The angle $\\theta(t)$ is taken with reference to the stable at-rest point with the mass hanging downwards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This system can be described by\n",
    "$$\n",
    "    \\theta''(t) = \\frac{-g}{L} \\sin(\\theta(t)).\n",
    "$$\n",
    "We will take $\\frac{g}{L} = 1$ for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking at the Taylor series of $\\sin$ we can approximate this equation for small $\\theta$ as\n",
    "$$\n",
    "    \\sin(\\theta) \\approx \\theta - \\frac{\\theta^3}{6} + \\mathcal{O}(\\theta^5)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "    \\theta'' = -\\theta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that this equation has solutions of the form\n",
    "$$\n",
    "    \\theta(t) = C_1 \\cos t + C_2 \\sin t.\n",
    "$$\n",
    "We clearly need two boundary conditions to uniquely specify the system which can be a bit awkward given that we usually specify these at two points in the spatial domain.  Since we are in time we can specify the initial position of the pendulum $\\theta(0) = \\alpha$ however the second condition would specify where the pendulum would be sometime in the future, say $\\theta(T) = \\beta$.  We could also specify another initial condition such as the angular velocity $\\theta'(0) = \\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Simple linear pendulum solutions\n",
    "def linear_pendulum(t, alpha=0.01, beta=0.01, T=1.0):\n",
    "    C_1 = alpha\n",
    "    C_2 = (beta - alpha * numpy.cos(T)) / numpy.sin(T)\n",
    "    return C_1 * numpy.cos(t) + C_2 * numpy.sin(t)\n",
    "\n",
    "\n",
    "alpha = [0.1, -0.1, -1.0]\n",
    "beta = [0.1, 0.1, 0.0]\n",
    "T = [1.0, 1.0, 1.0]\n",
    "t = numpy.linspace(0, 10.0, 100)\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "for i in range(len(alpha)):\n",
    "    axes.plot(t, linear_pendulum(t, alpha[i], beta[i], T[i]))\n",
    "axes.set_title(\"Solutions to the Linear Pendulum Problem\")\n",
    "axes.set_xlabel(\"t\")\n",
    "axes.set_ylabel(\"$\\theta$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But how would we go about handling the fully non-linear problem?  First let's discretize using our approach to date with the second order, centered second derivative finite difference approximation to find\n",
    "$$\n",
    "    \\frac{1}{\\Delta t^2}(\\theta_{i+1} - 2 \\theta_i + \\theta_{i-1}) + \\sin (\\theta_i) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most common approach to solving a non-linear BVP like this (and many non-linear PDEs for that matter) is to use Newton's method.  Recall that if we have a non-linear function $G(\\theta)$ and we want to find $\\theta$ such that\n",
    "$$\n",
    "    G(\\theta) = 0\n",
    "$$\n",
    "we can expand $G(\\theta)$ in a Taylor series to find\n",
    "$$\n",
    "    G(\\theta^{[k+1]}) = G(\\theta^{[k]}) + G'(\\theta^{[k]}) (\\theta^{[k+1]} - \\theta^{[k]}) + \\mathcal{O}((\\theta^{[k+1]} - \\theta^{[k]})^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we want $G(\\theta^{[k+1]}) = 0$ we can set this in the expression above (this is also known as a fixed point iteration) and dropping the higher order terms we can solve for $\\theta^{[k+1]}$ to find\n",
    "$$\\begin{aligned}\n",
    "    0 &= G(\\theta^{[k]}) + G'(\\theta^{[k]}) (\\theta^{[k+1]} - \\theta^{[k]} )\\\\\n",
    "    G'(\\theta^{[k]}) \\theta^{[k+1]} &= G'(\\theta^{[k]}) \\theta^{[k]} - G(\\theta^{[k]})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At this point we need to be careful, if we have a system of equations we cannot simply divide through by $G'(\\theta^{[k]})$ (which is now a matrix) to find our new value $\\theta^{[k+1]}$.  Instead we need to invert the matrix $G'(\\theta^{[k]})$.  Another way to write this is as an update to the value $\\theta^{[k+1]}$ where\n",
    "$$\n",
    "    \\theta^{[k+1]} = \\theta^{[k]} + \\delta^{[k]}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    J(\\theta^{[k]}) \\delta^{[k]} = -G(\\theta^{[k]}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we have introduced notation for the **Jacobian matrix** whose elements are\n",
    "$$\n",
    "    J_{ij}(\\theta) = \\frac{\\partial}{\\partial \\theta_j} G_i(\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So how do we compute the Jacobian matrix?  Since we know the system of equations in this case we can write down in general what the entries of $J$ are.\n",
    "$$\n",
    "    \\frac{1}{\\Delta t^2}(\\theta_{i+1} - 2 \\theta_i + \\theta_{i-1}) + \\sin (\\theta_i) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    J_{ij}(\\theta) = \\left \\{ \\begin{aligned}\n",
    "        &\\frac{1}{\\Delta t^2} & & j = i - 1, j = i + 1 \\\\\n",
    "        -&\\frac{2}{\\Delta t^2} + \\cos(\\theta_i) & & j = i \\\\\n",
    "        &0 & & \\text{otherwise}\n",
    "    \\end{aligned} \\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the Jacobian in hand we can solve the BVP by iterating until some stopping criteria is met (we have converged to our satisfaction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Solve the linear and non-linear pendulum problem with $T=2\\pi$, $\\alpha = \\beta = 0.7$.\n",
    " - Does the linear equation have a unique solution\n",
    " - Do you expect the original problem to have a unique solution (i.e. does the non-linear problem have a unique solution)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_nonlinear_pendulum(m, alpha, beta, T, max_iterations=100, tolerance=1e-3, verbose=False):\n",
    "    \n",
    "    # Discretization\n",
    "    t_bc = numpy.linspace(0.0, T, m + 2)\n",
    "    t = t_bc[1:-1]\n",
    "    delta_t = T / (m + 1)\n",
    "    diagonal = numpy.ones(t.shape)\n",
    "    G = numpy.empty(t_bc.shape)\n",
    "    \n",
    "    # Initial guess\n",
    "    theta = 0.7 * numpy.cos(t_bc)\n",
    "    theta[0] = alpha\n",
    "    theta[-1] = beta\n",
    "    \n",
    "    # Main iteration loop\n",
    "    success = False\n",
    "    for num_step in range(1, max_iterations):\n",
    "        \n",
    "        # Construct Jacobian matrix\n",
    "        J = numpy.diag(diagonal * -2.0 / delta_t**2 + numpy.cos(theta[1:-1]), 0)\n",
    "        J += numpy.diag(diagonal[:-1] / delta_t**2, -1)\n",
    "        J += numpy.diag(diagonal[:-1] / delta_t**2, 1)\n",
    "        \n",
    "        # Construct vector G\n",
    "        G = (theta[:-2] - 2.0 * theta[1:-1] + theta[2:]) / delta_t**2 + numpy.sin(theta[1:-1])\n",
    "        \n",
    "        # Take care of BCs\n",
    "        G[0] = (alpha - 2.0 * theta[1] + theta[2]) / delta_t**2 + numpy.sin(theta[1])\n",
    "        G[-1] = (theta[-3] - 2.0 * theta[-2] + beta) / delta_t**2 + numpy.sin(theta[-2])\n",
    "        \n",
    "        # Solve\n",
    "        delta = numpy.linalg.solve(J, -G)\n",
    "        theta[1:-1] += delta\n",
    "        \n",
    "        if verbose:\n",
    "            print(\" (%s) Step size: %s\" % (num_step, numpy.linalg.norm(delta)))\n",
    "        \n",
    "        if numpy.linalg.norm(delta) < tolerance:\n",
    "            success = True\n",
    "            break\n",
    "            \n",
    "    if not success:\n",
    "        print(numpy.linalg.norm(delta))\n",
    "        raise ValueError(\"Reached maximum allowed steps before convergence criteria met.\")\n",
    "    \n",
    "    return t_bc, theta\n",
    "\n",
    "t, theta = solve_nonlinear_pendulum(100, 0.7, 0.7, 2.0 * numpy.pi, tolerance=1e-9, verbose=True)\n",
    "plt.plot(t, theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Problem\n",
    "alpha = 0.7\n",
    "beta = 0.7\n",
    "T = 2.0 * numpy.pi\n",
    "t = numpy.linspace(0, T, 100)\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t, linear_pendulum(t, alpha, beta, T), 'r-', label=\"Linear\")\n",
    "\n",
    "# Non-linear problem\n",
    "t, theta = solve_nonlinear_pendulum(100, alpha, beta, T)\n",
    "axes.plot(t, theta, 'b-', label=\"Non-Linear\")\n",
    "\n",
    "axes.set_title(\"Solutions to the Pendulum Problem\")\n",
    "axes.set_xlabel(\"t\")\n",
    "axes.set_ylabel(\"$\\theta$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Note that there are two different ideas of convergence going on in our non-linear solver above, one is the convergence of the finite difference approximation controlled by $\\Delta x$ and the convergence of the Newton iteration.  We expect both to be second order (Newton's method converges quadratically under suitable assumptions).  How do these two methods combine to affect the global error though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First let's compute the LTE\n",
    "$$\\begin{aligned}\n",
    "    \\tau_{i} &= \\frac{1}{\\Delta t^2} (\\theta(t_{i+1}) - 2 \\theta(t_i) + \\theta(t_{i-1})) + \\sin \\theta(t_i) \\\\\n",
    "    &= \\frac{1}{\\Delta t^2} \\left (\\theta(t_i) + \\theta'(t_i) \\Delta t + \\frac{1}{2} \\theta''(t_i) \\Delta t^2 + \\frac{1}{6} \\theta'''(t_i) \\Delta t^3 + \\frac{1}{24} \\theta^{(4)}(t_i) \\Delta t^4 - 2 \\theta(t_i) \\right .\\\\\n",
    "    &\\quad \\quad \\quad \\left . + \\theta(t_i) - \\theta'(t_i) \\Delta t + \\frac{1}{2} \\theta''(t_i) \\Delta t^2 - \\frac{1}{6} \\theta'''(t_i) \\Delta t^3 + \\frac{1}{24} \\theta^{(4)}(t_i) \\Delta t^4 + \\mathcal{O}(\\Delta t^5) \\right) + \\sin \\theta(t_i) \\\\\n",
    "    &= \\frac{1}{\\Delta t^2} \\left (\\theta''(t_i) \\Delta t^2 + \\frac{1}{12} \\theta^{(4)}(t_i) \\Delta t^4 \\mathcal{O}(\\Delta t^6) \\right) + \\sin \\theta(t_i) \\\\\n",
    "    &= \\theta''(t_i)  + \\sin \\theta(t_i) + \\frac{1}{12} \\theta^{(4)}(t_i) \\Delta t^2 + \\mathcal{O}(\\Delta t^4).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Newton's method we can consider the difference of taking a step with the true solution to the BVP $\\hat{\\theta}$ vs. the approximate solution $\\theta$.  We can formulate an analogous LTE where\n",
    "$$\n",
    "    G(\\Theta) = 0 \\quad G(\\hat{\\Theta}) = \\tau.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Following our discussion from before we can use these two expressions to find\n",
    "$$\n",
    "    G(\\Theta) - G(\\hat{\\Theta}) = -\\tau\n",
    "$$\n",
    "and from here we want to derive an expression of the global error $E = \\Theta - \\hat{\\Theta}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since $G(\\theta)$ is not linear we will write the above expression as a Taylor series to find\n",
    "$$\n",
    "    G(\\Theta) = G(\\hat{\\Theta}) + J(\\hat{\\Theta}) E + \\mathcal{O}(||E||^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using this expression we find\n",
    "$$\n",
    "    J(\\hat{\\Theta}) E = -\\tau + \\mathcal{O}(||E||^2).\n",
    "$$\n",
    "Ignoring higher order terms then we have a linear expression for $E$ which we can solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This motivates another definition of stability then involving the Jacobian of $G$.  The nonlinear difference methods $G(\\Theta) = 0$ is *stable* in some norm $||\\cdot||$ if the matrices $(J_{\\Delta t})^{-1}$ are uniformly bounded in the norm as $\\Delta t \\rightarrow 0$.  In other words $\\exists C$ and $\\Delta t^0$ s.t.\n",
    "$$\n",
    "    ||(J_{\\Delta t})^{-1}|| \\leq C \\quad \\forall \\Delta t < \\Delta t^0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given this sense of stability and consistency ($||\\tau|| \\rightarrow 0$ as $\\Delta t \\rightarrow 0$) then the method converges as\n",
    "$$\n",
    "    ||E_{\\Delta t}|| \\rightarrow 0 \\quad \\text{as} \\quad \\Delta t \\rightarrow 0.\n",
    "$$\n",
    "\n",
    "Note that we are still not guaranteed that Newton's method will converge, say from a bad initial guess, even though we have shown convergence.  It can be proven that Newton's method will converge from a sufficiently good initial guess.  It also should be noted that although Newton's method may have an error that is to round-off does not imply that the error will follow suit."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
